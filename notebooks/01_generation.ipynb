{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/KIMNAMHYEON-Kpass/love-letter-lab/blob/main/notebooks/01_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 연애편지 생성을 위한 LLM 프롬프트 엔지니어링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 노트북은 Hugging Face의 언어 모델을 사용하여 주어진 조건에 맞는 연애편지를 생성하는 과정을 담고 있습니다. \n",
        "실행 순서는 다음과 같습니다.\n",
        "1. **설치**: 필요한 라이브러리를 설치합니다.\n",
        "2. **모델 로드**: SKT-AI의 KoGPT2 모델을 불러옵니다.\n",
        "3. **프롬프트 정의**: 편지 내용의 가이드라인이 될 프롬프트를 설정합니다.\n",
        "4. **생성**: 정의된 프롬프트를 기반으로 모델이 편지를 생성합니다.\n",
        "5. **검증**: 생성된 편지가 요구사항을 충족하는지 확인합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. 설치 (Installation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "!pip install transformers accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. 모델 로드 (Model Loading)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"skt/kogpt2-base-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. 프롬프트 정의 (Prompt Definition)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# 편지 생성 프롬프트 변수 정의\n",
        "name = \"민수\"\n",
        "relation = \"동기\"\n",
        "situation = \"발표를 앞두고 긴장한 상태\"\n",
        "tone = \"달달\"\n",
        "length = 3  # 생성할 문장 수\n",
        "\n",
        "prompt = f\"\"\"\n",
        "당신은 연애 편지 작가입니다.\n",
        "[제약 조건]\n",
        "- 반드시 존댓말로 작성합니다.\n",
        "- 과장된 비유나 지나친 감정 표현을 피합니다.\n",
        "- 톤은 {tone}로 유지합니다.\n",
        "- 총 {length} 문장으로 작성합니다.\n",
        "- 느낌표는 최대 1번만 사용합니다.\n",
        "- 이모지는 사용하지 않습니다.\n",
        "[상황 정보]\n",
        "- 받는 사람 이름: {name}\n",
        "- 관계: {relation}\n",
        "- 상황: {situation}\n",
        "위 정보를 모두 반영하여 자연스럽고 따뜻한 연애 편지를 작성하세요.\n",
        "편지는 자연스러운 문단으로 출력하고, 인사말이나 서명은 넣지 마세요.\n",
        "\"\"\"\n",
        "print(\"--- 프롬프트 미리보기 ---\")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. 생성 (Generation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# 하이퍼파라미터를 조절해 생성\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# 문장당 30~50 토큰으로 가정하여 최대 토큰 수 설정\n",
        "max_tokens = length * 50\n",
        "\n",
        "output_sequences = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=max_tokens,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    do_sample=True,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# 생성된 텍스트에서 프롬프트 부분 제거\n",
        "full_generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "generated_letter = full_generated_text.replace(prompt, \"\").strip()\n",
        "\n",
        "print(\"--- 생성된 편지 ---\")\n",
        "print(generated_letter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. 검증 (Verification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# 생성 결과 검증\n",
        "print(\"--- 생성 결과 검증 ---\")\n",
        "\n",
        "# 검증 항목 1: 문장 수\n",
        "# 간단하게 마침표(.) 개수로 문장 수를 근사합니다.\n",
        "num_sentences = generated_letter.count('.') + generated_letter.count('!') + generated_letter.count('?')\n",
        "print(f\"요구 문장 수: {length}\")\n",
        "print(f\"생성 문장 수: {num_sentences}\")\n",
        "if num_sentences >= length:\n",
        "    print(\"-> 문장 수 검증: [PASS]\")\n",
        "else:\n",
        "    print(\"-> 문장 수 검증: [FAIL]\")\n",
        "\n",
        "# 검증 항목 2: 금칙어 포함 여부\n",
        "forbidden_words = [\"이모지\", \"스맥다운\", \"게임\"] # 프롬프트에 금지된 단어 및 생성 시 나타난 부적절한 단어\n",
        "found_forbidden = False\n",
        "for word in forbidden_words:\n",
        "    if word in generated_letter:\n",
        "        print(f\"-> 금칙어 '{word}' 발견: [FAIL]\")\n",
        "        found_forbidden = True\n",
        "        break\n",
        "if not found_forbidden:\n",
        "    print(\"-> 금칙어 검증: [PASS]\")\n",
        "\n",
        "# 검증 항목 3: 이름 포함 여부\n",
        "if name in generated_letter:\n",
        "    print(f\"-> 이름 '{name}' 포함 검증: [PASS]\")\n",
        "else:\n",
        "    print(f\"-> 이름 '{name}' 포함 검증: [FAIL]\")"
      ]
    }
  ]
}